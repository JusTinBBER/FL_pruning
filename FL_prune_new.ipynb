{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#套件\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "from typing import Tuple, Dict\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "from torchvision.datasets import CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaasda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub a and b  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centralized PyTorch training\n",
      "Load data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Start training\n",
      "Evaluate model\n",
      "Loss:  721.453800201416\n",
      "Accuracy:  0.1016\n"
     ]
    }
   ],
   "source": [
    "#模型架構\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "\n",
    "def load_data() -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, Dict]:\n",
    "    \"\"\"Load CIFAR-10 (training and test set).\"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(DATA_ROOT, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    testset = CIFAR10(DATA_ROOT, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "    num_examples = {\"trainset\" : len(trainset), \"testset\" : len(testset)}\n",
    "    return trainloader, testloader, num_examples\n",
    "\n",
    "def train(\n",
    "    net: Net,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"Train the network.\"\"\"\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    print(f\"Training {epochs} epoch(s) w/ {len(trainloader)} batches each\")\n",
    "\n",
    "    # Train the network\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "def test(\n",
    "    net: Net,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Centralized PyTorch training\")\n",
    "    print(\"Load data\")\n",
    "    trainloader, testloader, _ = load_data()\n",
    "    print(\"Start training\")\n",
    "    net=Net().to(DEVICE)\n",
    "    # train(net=net, trainloader=trainloader, epochs=2, device=DEVICE)\n",
    "    print(\"Evaluate model\")\n",
    "    loss, accuracy = test(net=net, testloader=testloader, device=DEVICE)\n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "#efficient模型架構\n",
    "\n",
    "def swish(x):\n",
    "    return x * x.sigmoid()\n",
    "\n",
    "\n",
    "def drop_connect(x, drop_ratio):\n",
    "    keep_ratio = 1.0 - drop_ratio\n",
    "    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n",
    "    mask.bernoulli_(keep_ratio)\n",
    "    x.div_(keep_ratio)\n",
    "    x.mul_(mask)\n",
    "    return x\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    '''Squeeze-and-Excitation block with Swish.'''\n",
    "\n",
    "    def __init__(self, in_channels, se_channels):\n",
    "        super(SE, self).__init__()\n",
    "        self.se1 = nn.Conv2d(in_channels, se_channels,\n",
    "                             kernel_size=1, bias=True)\n",
    "        self.se2 = nn.Conv2d(se_channels, in_channels,\n",
    "                             kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        out = swish(self.se1(out))\n",
    "        out = self.se2(out).sigmoid()\n",
    "        out = x * out\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''expansion + depthwise + pointwise + squeeze-excitation'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 expand_ratio=1,\n",
    "                 se_ratio=0.,\n",
    "                 drop_rate=0.):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.expand_ratio = expand_ratio\n",
    "\n",
    "        # Expansion\n",
    "        channels = expand_ratio * in_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels,\n",
    "                               channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Depthwise conv\n",
    "        self.conv2 = nn.Conv2d(channels,\n",
    "                               channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=(1 if kernel_size == 3 else 2),\n",
    "                               groups=channels,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # SE layers\n",
    "        se_channels = int(in_channels * se_ratio)\n",
    "        self.se = SE(channels, se_channels)\n",
    "\n",
    "        # Output\n",
    "        self.conv3 = nn.Conv2d(channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Skip connection if in and out shapes are the same (MV-V2 style)\n",
    "        self.has_skip = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x if self.expand_ratio == 1 else swish(self.bn1(self.conv1(x)))\n",
    "        out = swish(self.bn2(self.conv2(out)))\n",
    "        out = self.se(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.has_skip:\n",
    "            if self.training and self.drop_rate > 0:\n",
    "                out = drop_connect(out, self.drop_rate)\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, cfg, num_classes=10):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.conv1 = nn.Conv2d(3,\n",
    "                               32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_channels=32)\n",
    "        self.linear = nn.Linear(cfg['out_channels'][-1], num_classes)\n",
    "\n",
    "    def _make_layers(self, in_channels):\n",
    "        layers = []\n",
    "        cfg = [self.cfg[k] for k in ['expansion', 'out_channels', 'num_blocks', 'kernel_size',\n",
    "                                     'stride']]\n",
    "        b = 0\n",
    "        blocks = sum(self.cfg['num_blocks'])\n",
    "        for expansion, out_channels, num_blocks, kernel_size, stride in zip(*cfg):\n",
    "            strides = [stride] + [1] * (num_blocks - 1)\n",
    "            for stride in strides:\n",
    "                drop_rate = self.cfg['drop_connect_rate'] * b / blocks\n",
    "                layers.append(\n",
    "                    Block(in_channels,\n",
    "                          out_channels,\n",
    "                          kernel_size,\n",
    "                          stride,\n",
    "                          expansion,\n",
    "                          se_ratio=0.25,\n",
    "                          drop_rate=drop_rate))\n",
    "                in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = swish(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        dropout_rate = self.cfg['dropout_rate']\n",
    "        if self.training and dropout_rate > 0:\n",
    "            out = F.dropout(out, p=dropout_rate)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def EfficientNetB0():\n",
    "    cfg = {\n",
    "        'num_blocks': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'expansion': [1, 6, 6, 6, 6, 6, 6],\n",
    "        'out_channels': [16, 24, 40, 80, 112, 192, 320],\n",
    "        'kernel_size': [3, 3, 5, 3, 5, 5, 3],\n",
    "        'stride': [1, 2, 2, 2, 1, 2, 1],\n",
    "        'dropout_rate': 0.2,\n",
    "        'drop_connect_rate': 0.2,\n",
    "    }\n",
    "    return EfficientNet(cfg)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = EfficientNetB0()\n",
    "    x = torch.randn(2, 3, 32, 32)\n",
    "    y = net(x)\n",
    "    print(y.shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "#模型剪枝\n",
    "\n",
    "# Let's load the model we just created and test the accuracy per label\n",
    "model = EfficientNetB0()\n",
    "state_dict = torch.load('/Users/taka/Desktop/code/FL-Pruning/FL3/efficientnet-b0.pth',map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "layer_name = []\n",
    "\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "name_list = [n for n,p in model.named_parameters()]\n",
    "#print(name_list)\n",
    "\n",
    "for name in name_list:\n",
    "\n",
    "    # layer_name.append(name)\n",
    "    # print(name)\n",
    "    if 'weight' in name:\n",
    "        m = getattr(model, name.split('.')[0])\n",
    "        # print(m)\n",
    "\n",
    "        try:\n",
    "            prune.ln_structured(m,name=\"weight\", amount=0.2,n=1,dim=0)\n",
    "            a = m.weight.data\n",
    "            b = m.weight_mask.data\n",
    "\n",
    "            m.weight.data =m.weight.data.mul(m.weight_mask.data)       \n",
    "            c = m.weight_mask.data\n",
    "\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                    print(module)\n",
    "                    prune.remove(module, 'weight')\n",
    "                    # prune.remove(module, 'bias')\n",
    "        except:\n",
    "            try:\n",
    "                \n",
    "                prune.remove(module, 'weigh_mask')\n",
    "                print('a')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "#原始code---------------------------------\n",
    "\n",
    "\n",
    "        # a = m.weight.data\n",
    "    \n",
    "        # b = m.weight_mask.data\n",
    "        # print(a[0][2])\n",
    "        # print(b[0][2])\n",
    "\n",
    "\n",
    "        #sub weight to mask \n",
    "        \n",
    "        # m.weight.data =m.weight.data.mul(m.weight_mask.data)       \n",
    "        # c = m.weight_mask.data\n",
    "        # print(c[0][2])\n",
    "\n",
    "\n",
    "        # get the weight of each layer\n",
    "    # if 'bias' in name:\n",
    "    #     try:\n",
    "    #         m = getattr(model, name.split('.')[0])\n",
    "    #         prune.random_unstructured(m,name=\"bias\", amount=0.03)\n",
    "\n",
    "    #         m.bias.data =m.bias.data.mul(m.bias_mask.data)\n",
    "\n",
    "    #         #sub weight and mask        \n",
    "    #         weight = m.bias.data\n",
    "    #         mask = m.bias_mask.data\n",
    "    #     except:\n",
    "    #         pass\n",
    "\n",
    "#prune model mask   \n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "#         print(module)\n",
    "#         prune.remove(module, 'weight')\n",
    "#         # prune.remove(module, 'bias')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # save model\n",
    "torch.save(model.state_dict(), '/Users/taka/Desktop/code/FL-Pruning/FL3/efficientnet-b0_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight_orig\n",
      "linear.weight_mask\n",
      "Files already downloaded and verified\n",
      "Accuracy of the network on the 10000 test images: 23 %\n"
     ]
    }
   ],
   "source": [
    "# vaildation the model\n",
    "model = EfficientNetB0()\n",
    "state_dict = torch.load('/Users/taka/Desktop/code/FL-Pruning/FL3/efficientnet-b0_new.pth',map_location='cpu')\n",
    "\n",
    "#新--------------------------------\n",
    "from collections import OrderedDict \n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for key,value in torch.load('/Users/taka/Desktop/code/FL-Pruning/FL3/efficientnet-b0_new.pth').items():\n",
    "    if 'weight_orig' in key:\n",
    "        title = key.split('.')\n",
    "        name = title[0]+'.weight'\n",
    "        new_state_dict[name] = value\n",
    "        value1 = value\n",
    "        print(key)\n",
    "    elif 'weight_mask' in key:\n",
    "        title = key.split('.')\n",
    "        name = title[0]+'.weight'\n",
    "        new_state_dict[name] = value1\n",
    "        print(key)\n",
    "    else:\n",
    "        new_state_dict[key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "#原始--------------------------------\n",
    "#model.load_state_dict(state_dict)\n",
    "#--------------------------------\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# generate the test data\n",
    "testset = CIFAR10(DATA_ROOT, train=False, download=True, transform=transform)\n",
    "# generate the test data\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0], data[1]\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight_orig\n",
      "linear.weight_mask\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vaildation the model\n",
    "model = EfficientNetB0()\n",
    "state_dict = torch.load('/Users/taka/Desktop/code/FL-Pruning/FL3/efficientnet-b0_74%_new.pth',map_location='cpu')\n",
    "\n",
    "#新--------------------------------\n",
    "from collections import OrderedDict \n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for key,value in torch.load('/Users/taka/Desktop/code/FL-Pruning/FL3/efficientnet-b0_74%_new\\\n",
    "    .pth',map_location='cpu').items():\n",
    "    if 'weight_orig' in key:\n",
    "        title = key.split('.')\n",
    "        name = title[0]+'.weight'\n",
    "        new_state_dict[name] = value\n",
    "        value1 = value\n",
    "        print(key)\n",
    "    elif 'weight_mask' in key:\n",
    "        title = key.split('.')\n",
    "        name = title[0]+'.weight'\n",
    "        new_state_dict[name] = value1\n",
    "        print(key)\n",
    "    else:\n",
    "        new_state_dict[key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "908abd7e78fd4d71ba1be92795635fd82be5080a16e3cc7c1eae8bbfec458fa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
